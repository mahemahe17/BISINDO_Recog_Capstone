{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f864c0a9-d9a9-42ad-9715-544c6616fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45967593-6175-463e-809a-02a7fb8daa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIC_LABELS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', \n",
    "                 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', \n",
    "                 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "DYNAMIC_LABELS = ['AKU', 'HALO', 'APA', 'KABAR', 'KAMU', 'NAMA', 'PERKENALKAN', \n",
    "                  'SIAPA', 'TERIMA KASIH', 'TOLONG', 'SALAM KENAL', 'MAAF', \n",
    "                  'BERASAL', 'DARI', 'MANA', 'SAMA-SAMA', 'YA', 'TIDAK', 'JAKARTA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53877aed-0eb9-421a-a0f2-f09e52817316",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47eade1b-5346-411b-bf55-4999ebfdd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results, include_pose=True):\n",
    "    pose = [[res.x, res.y] for res in results.pose_landmarks.landmark] if results.pose_landmarks else [[0, 0]] * 33\n",
    "    lh = [[res.x, res.y] for res in results.left_hand_landmarks.landmark] if results.left_hand_landmarks else [[0, 0]] * 21\n",
    "    rh = [[res.x, res.y] for res in results.right_hand_landmarks.landmark] if results.right_hand_landmarks else [[0, 0]] * 21\n",
    "\n",
    "    if include_pose:\n",
    "        return np.array(pose + lh + rh).flatten()\n",
    "    else:\n",
    "        return np.array(lh + rh).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20857adc-4d84-4812-9f24-18fa3fdd010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = holistic.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "957860e1-559a-4e01-87b5-5b2223b6ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results, include_pose=True):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    if include_pose:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23e3a286-3ba1-4c1c-94db-e1836d7f3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints):\n",
    "    keypoints = np.array(keypoints).reshape(-1, 2)\n",
    "    min_val = keypoints.min(axis=0)\n",
    "    max_val = keypoints.max(axis=0)\n",
    "    range_val = np.where((max_val - min_val) == 0, 1, (max_val - min_val))\n",
    "    normed = (keypoints - min_val) / range_val\n",
    "    return normed.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55c563b6-5054-4653-bbdf-a948b1df05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "081e7a7a-75b5-4404-9ac9-a304e97d86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_movement(curr, prev):\n",
    "    if prev is None:\n",
    "        return 0.0\n",
    "    return np.linalg.norm(curr - prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be831112-eaf8-4a25-92d4-e1933a4e6c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_static = tf.keras.models.load_model(\"static_model.h5\")\n",
    "model_dynamic = tf.keras.models.load_model(\"high_dynamic_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73de0cf6-85eb-4ef7-8e75-646459e46329",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30\n",
    "MOVEMENT_THRESHOLD = 0.04\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "silence_timeout = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c0cc31d-a277-4bbf-a53b-2f643c7d365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "prev_keypoints = None\n",
    "movement_history = deque(maxlen=5)\n",
    "frame_counter = 0\n",
    "last_spoken = \"\"\n",
    "last_speak_time = 0\n",
    "history = deque(maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "578a9a3c-54e1-4278-b9bf-d1d1776fbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_counter += 1\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, include_pose=True)\n",
    "\n",
    "        # Cek apakah tangan terdeteksi\n",
    "        if not results.left_hand_landmarks and not results.right_hand_landmarks:\n",
    "            cv2.putText(image, 'No hand detected', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            cv2.imshow('BISINDO Gesture Recognition', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Ekstraksi dan normalisasi keypoint\n",
    "        keypoints_static = extract_keypoints(results, include_pose=False)\n",
    "        keypoints_dynamic = extract_keypoints(results, include_pose=True)\n",
    "        norm_static = normalize_keypoints(keypoints_static)\n",
    "        norm_dynamic = normalize_keypoints(keypoints_dynamic)\n",
    "\n",
    "        # Hitung pergerakan\n",
    "        movement = compute_movement(norm_dynamic, prev_keypoints)\n",
    "        movement_history.append(movement)\n",
    "        prev_keypoints = norm_dynamic\n",
    "        is_dynamic = np.mean(movement_history) > MOVEMENT_THRESHOLD\n",
    "\n",
    "        # Inisialisasi hasil\n",
    "        final_pred = \"-\"\n",
    "        final_conf = 0.0\n",
    "        label_static, conf_static = \"-\", 0.0\n",
    "        label_dynamic, conf_dynamic = \"-\", 0.0\n",
    "\n",
    "        # ======= PREDIKSI =========\n",
    "        if is_dynamic:\n",
    "            sequence.append(norm_dynamic)\n",
    "            if len(sequence) > SEQUENCE_LENGTH:\n",
    "                sequence = sequence[-SEQUENCE_LENGTH:]\n",
    "            if len(sequence) == SEQUENCE_LENGTH and frame_counter % 10 == 0:\n",
    "                input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "                pred_dynamic = model_dynamic.predict(input_seq, verbose=0)[0]\n",
    "                conf_dynamic = np.max(pred_dynamic)\n",
    "                if conf_dynamic > CONFIDENCE_THRESHOLD:\n",
    "                    label_dynamic = DYNAMIC_LABELS[np.argmax(pred_dynamic)]\n",
    "                    final_pred = label_dynamic\n",
    "                    final_conf = conf_dynamic\n",
    "                    history.append(label_dynamic)\n",
    "                    if len(history) == history.maxlen:\n",
    "                        final_pred = max(set(history), key=history.count)\n",
    "                        history.clear()\n",
    "        else:\n",
    "            pred_static = model_static.predict(np.expand_dims(norm_static, axis=0), verbose=0)[0]\n",
    "            conf_static = np.max(pred_static)\n",
    "            if conf_static > CONFIDENCE_THRESHOLD:\n",
    "                label_static = STATIC_LABELS[np.argmax(pred_static)]\n",
    "                final_pred = label_static\n",
    "                final_conf = conf_static\n",
    "\n",
    "        # === Bicara jika gesture berbeda dan timeout terpenuhi\n",
    "        if final_pred != \"-\" and final_pred != last_spoken:\n",
    "            if time.time() - last_speak_time > silence_timeout:\n",
    "                speak(final_pred)\n",
    "                last_spoken = final_pred\n",
    "                last_speak_time = time.time()\n",
    "\n",
    "        # === UI Tampilan\n",
    "        cv2.rectangle(image, (0, 0), (640, 90), (245, 245, 245), -1)\n",
    "        cv2.putText(image, f'STATIC: {label_static} ({conf_static:.2f})', (10, 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(image, f'DYNAMIC: {label_dynamic} ({conf_dynamic:.2f})', (10, 55),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 128, 0), 2)\n",
    "        cv2.putText(image, f'FINAL: {final_pred} ({final_conf:.2f})', (10, 85),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 0, 128), 2)\n",
    "\n",
    "        cv2.imshow('BISINDO Gesture Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
